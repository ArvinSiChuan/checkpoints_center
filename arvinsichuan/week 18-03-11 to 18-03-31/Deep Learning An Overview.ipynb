{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning An Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Metworks and Deep Learning\n",
    "\n",
    "### Commonly Seen Networks\n",
    "1. Standard Nueral Network\n",
    "2. CNN\n",
    "3. RNN\n",
    "4. Custome Hybtid\n",
    "\n",
    "### Logistic Regression  \n",
    "Given $x$, want  $\\hat{y} = P(y=1 \\mid x)$, $x \\in \\Re^{n_x}$  \n",
    "Parameters: $w \\in \\Re^(n_x), b \\in \\Re$  \n",
    "Output: $\\hat{y} = \\sigma (w^Tx+b)$\n",
    "$\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Loss(Error) Function    \n",
    "$L(\\hat{y}, y) = -[ylog\\hat{y}+(1-y)log(1-\\hat{y})]$\n",
    "Cost Function  \n",
    "$J(w,b) = \\frac{1}{m} \\sum_1^m L(\\hat{y}^{(i)},y) = \\frac{1}{m}\\sum_{1}^{m}[y^{(i)}log\\hat{y}^{(i)}+(1-y^{(i)})log(1-\\hat{y}^{(i)})]$\n",
    "\n",
    "### GD(Gradient Descent)  \n",
    "Repeadly{  \n",
    "    $w:=w - \\alpha\\frac{dJ(w)}{dw}$ \n",
    "}\n",
    "\n",
    "### Normalizing  \n",
    "$x_normalized =  \\frac{x}{\\left \\| x \\right \\|}$  \n",
    "\n",
    "\n",
    "### Softmax  \n",
    "for $x \\in \\Re^{'(xn)}, softmax(x) = softmax([x_1,... ,x_n])$\n",
    "$= [\\frac{e^{x_1}}{\\sum_{j}e^{x_j}},... ,\\frac{e^{x_n}}{\\sum_{j}e^{x_j}}]$ \n",
    "\n",
    "### Useful `np` Functions  \n",
    "```python\n",
    "# All are vectorization supported\n",
    "# e^x\n",
    "np.exp(x) \n",
    "\n",
    "# ||x||\n",
    "np.linalg.norm(x,axis=1,keepdim=True)\n",
    "\n",
    "# sum\n",
    "np.sum(x,axis=1,keepdim=True)\n",
    "\n",
    "# dot product\n",
    "np.dot(x,y)\n",
    "\n",
    "# outer product\n",
    "np.outer(x,y)\n",
    "\n",
    "# mathematical numerical multiply\n",
    "np.multiply(x,y)\n",
    "\n",
    "# reshape\n",
    "np.reshape(row,col)\n",
    "```\n",
    "\n",
    "### Common Steps for pre-processing a new dataset are:  \n",
    "1. Figure out the dimensions and the shapes of the problem(m_trian, m_test, num_px,...)\n",
    "2. Reshape the datasets such that each example is now a vector of size(num_px\\*num_px\\*3,1)\n",
    "3. Standardize the data.(Images can be applied by being devided by 255, which is the length of the color area)\n",
    "\n",
    "### To Implement a NN:  \n",
    "1. Initialize(w,b)\n",
    "2. Optimize the loss iteratively to learn parameters such as (w,b)   \n",
    "    - computing the cost and its gradient  \n",
    "    - updating the parameters using gtradient descent  \n",
    "3. Use the leart (w,b) to predict the labels for a given set of examples\n",
    "\n",
    "### For Better Algorithm effeciency and accuracy  \n",
    "1. Preprocessing the dataset is important\n",
    "2. Implement each function seperatedly and build a model together\n",
    "3. Tuning the learning rate (a kind of 'hyper-parameter') can make big difference to the algorithm\n",
    "\n",
    "### Shallow Neural Network  \n",
    "Repeatedly{\n",
    "$Z^{[i]} = W^{[i]}+b{[i]},$\n",
    "$A^{[i]} = \\sigma(Z^{[i]}ï¼‰$\n",
    "}\n",
    "\n",
    "### Activation Functions\n",
    "1. sigmoid:  \n",
    "$$\n",
    "a(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "2. tanh:  \n",
    "$$\n",
    "a(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
    "$$\n",
    "3. ReLU(Rectified Linear Unit):  \n",
    "$$\n",
    "    a(x) =max(0,x)\n",
    "$$\n",
    "4. Leaky ReLU:  \n",
    "$$\n",
    "a(x) = max(0.01*x,x)\n",
    "$$\n",
    "\n",
    "### Why do we need non-linear activation function?  \n",
    "If we use linear activation function, we just get a result of linear computation.\n",
    "\n",
    "### Derivatives of activation functions\n",
    "1. sigmoid:  \n",
    "$$\n",
    "g'(z)=g(z)[1-g(z)]\n",
    "$$\n",
    "2. ranh:\n",
    "$$\n",
    "g'(z)=1-[tan(z)]^2\n",
    "$$\n",
    "3. Relu:  \n",
    "$$\n",
    "g(z) = \n",
    "\\left\\{\\begin{matrix}\n",
    "0 & ,if\\ z < 0 \\\\ \n",
    "1 & ,if\\ z >0 \\\\\n",
    "undefined & ,if z=0 \n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "4. Leaky ReLU:\n",
    "$$\n",
    "g(z) = \n",
    "\\left\\{\\begin{matrix}\n",
    "0.01 & ,if\\ z < 0 \\\\ \n",
    "1 & ,if\\ z >0 \\\\\n",
    "undefined & ,if z=0 \n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "### General GD Algorithm\n",
    "1. Forward Propagation:\n",
    "$$\n",
    "Z^{[i]} = W^{[i]}X^{[i-1]} + b^{[i]},\\\\  \n",
    "A^{[i]} = g^{[i]}(Z^{[i]})\n",
    "$$\n",
    "2. Backward Propagation:  \n",
    "$$\n",
    "d_Z^{[i]} = W^{[i+1]T}d_z^{[i+1]} * g'^{[i]}(Z^{[i]})\\\\  \n",
    "d_w^{[i]} = \\frac{1}{m}d_z^{[i]}X^T\\\\ \n",
    "d_b^{[i]} = \\frac{1}{m}\\sum d_z^{[i]}\n",
    "$$\n",
    "3. Update Weights:\n",
    "$$\n",
    "W = W - \\alpha * d_W\\\\\n",
    "b = b - \\alpha * d_b\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Deep NN - Hyper-parameters tuning, Regularization and Optimization  \n",
    "\n",
    "### Setting Up Machine Learning Applications\n",
    "#### Hyper-parameters\n",
    "1. \\# layers\n",
    "2. \\# hidden layers\n",
    "3. learning rate\n",
    "4. activation functions\n",
    "\n",
    "| Condition   |Train Sets     | Dev Sets (Cross Validation)    | Test Sets(Optional)    |\n",
    "| :------------- | :------------- | :------------- |:------------- |\n",
    "| small amount of data     | 60      | 20    | 20    |\n",
    "| big data    | 98    | 1    | 1    |\n",
    "\n",
    "#### Make sure Dev and Test the same distribution.\n",
    "Test ensures how performance will be on the target so it is necessary to keep them the same distribution.\n",
    "\n",
    "#### Bias And Variance\n",
    "- Bias measures how the performance is between train set accuracy and the baseline(generally, will be human level performance).\n",
    "- Variance measures how the performance of Dev set compared with the Train set.\n",
    "\n",
    "#### Solution for high Bias and Variance\n",
    "1. High Bias:\n",
    "    - Bigger Network\n",
    "    - Train Longer\n",
    "    - NN architecture research\n",
    "2. High Variance:\n",
    "    - More Data\n",
    "    - Regularization\n",
    "    - NN architecture research\n",
    "\n",
    "### Regularization\n",
    "- $L_1$ Regularization:\n",
    "$$\n",
    "\\left \\| w \\right \\|^{2}_{2} = \\sum_{j = 1}^{n_x} w_j^2 = w^Tw\n",
    "$$\n",
    "\n",
    "- $L_2$ Regularization: \n",
    "$$\n",
    "\\frac{\\lambda}{2m}\\sum_{j = 1}^{n_x}|w_j|=\\frac{\\lambda}{2m}\\left \\| w \\right \\|_1\n",
    "$$\n",
    "- Frobenius Norm\n",
    "$$\n",
    "\\left \\| w^{[L]} \\right \\|^2_F = \\sum_{i = 1}^{n^{[L-1]}}|w_j|\\sum_{j = 1}^{n^{[L-1]}}(w_{ij})^2\n",
    "$$\n",
    "\n",
    "#### How does regularization prevent overfitting?\n",
    "Some of the hidden layer has been waken so that the overfitting has been modified, too.  \n",
    "Because of the regularization parameter to reduce the weights to more close to zero, so the real function is more close to a linear function.\n",
    "\n",
    "### Dropout Regularization\n",
    "#### What's a dropout?\n",
    "- Eliminating some hidden unit randomly\n",
    "- No dropout when making prediction \n",
    "\n",
    "#### Why does dropout works?\n",
    "- Overfitting is happening so we can't rely on any one feature, and thus we choose to spread out weights.  \n",
    "- To different Layer, choose relatively drop-out `keep_prov` value(usually not for input layer).\n",
    "- No overfitting, no dropout.\n",
    "\n",
    "### Other way to regularization\n",
    "- Data Augmentation\n",
    "    - flipping horizontally\n",
    "    - randomly distortion/zooming\n",
    "- Early Stopping\n",
    "\n",
    "### Orthogonalization \n",
    "1. Optimize cost function:\n",
    "    - Gradient descent\n",
    "    - Momentum\n",
    "    - RMS Prop\n",
    "    - Atom\n",
    "2. Non-overfit\n",
    "    - Regularization\n",
    "    - Getting more data\n",
    "3. Early stop can't make you handle two of above problem independently.\n",
    "\n",
    "4. Bias $\\to $ error rate & Variance $\\to$ overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
